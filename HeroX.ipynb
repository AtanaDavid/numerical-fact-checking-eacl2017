{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# HeroX Numerical Fact Checking System. Univeristy of Sheffield\n",
    "\n",
    "Pre-requisites:\n",
    " * Gradle\n",
    " * Java jdk8\n",
    " * Python 3\n",
    "  * numpy\n",
    "  * jnius\n",
    "  * fuzzywuzzy\n",
    "  * sklearn\n",
    "  * urllib3\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common setup\n",
    "\n",
    "Import required dependencies and download/install Stanford CoreNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "\n",
    "#Set path manually to incldue sources location\n",
    "if 'src/' not in sys.path:\n",
    "    sys.path.append('src/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the following step fails. Run `gradlew writeClasspath` on the terminal in this folder. Then try again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load Java classpath for stanford corenlp using gradle. this will also install it if missing\n",
    "from subprocess import run,PIPE\n",
    "if 'CLASSPATH' not in os.environ:\n",
    "    if not (os.path.exists('build') and os.path.exists('build/classpath.txt')):\n",
    "        print(\"Generating classpath\")\n",
    "        r=run([\"./gradlew\", \"writeClasspath\"],stdout=PIPE, stderr=PIPE, universal_newlines=True)\n",
    "        print(r.stdout)\n",
    "        print(r.stderr)\n",
    "              \n",
    "    print(\"Loading classpath\")\n",
    "    os.environ['CLASSPATH'] = open('build/classpath.txt','r').read()\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Optional] Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Search Engine Queries From Tables\n",
    "\n",
    "Run if a new table is added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from distant_supervision.query_generation import generate_queries\n",
    "from tabular.table_reader import read_table, number_tuples\n",
    "from wikitablequestions.dataset_reader import load_instances\n",
    "\n",
    "print(\"Generating Queries\")\n",
    "world = \"herox\"\n",
    "if os.path.exists(\"data/distant_supervision/queries_\"+world +\".txt\"):\n",
    "    print(\"Already done. No need to run again\")\n",
    "else:\n",
    "    all_instances = []\n",
    "    all_instances.extend(load_instances(world))\n",
    "    table_files = []\n",
    "\n",
    "    done = 0\n",
    "    for instance in all_instances:\n",
    "        table_files.append(instance['table'])\n",
    "\n",
    "    table_files = set(table_files)\n",
    "\n",
    "    with open(\"data/distant_supervision/queries_\"+world +\".txt\", \"w+\") as file:\n",
    "        for table_file in table_files:\n",
    "            done += 1\n",
    "            print(\"Parsing \" + str(done) +\"/\"+str(len(table_files)) + \"\\t\\t\\t\" + table_file)\n",
    "            table = number_tuples(read_table(table_file))\n",
    "            tuples = generate_queries(table)\n",
    "\n",
    "\n",
    "            for tuple in tuples:\n",
    "                file.write(table_file + \"\\t\" + tuple + \"\\n\")\n",
    "            file.flush()\n",
    "            os.fsync(file.fileno())\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download all web pages for queries\n",
    "\n",
    "We include the web pages with this submission as downloading the web pages takes considerable time. This script will not re-download web pages it already has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from distant_supervision.clean_html import get_text\n",
    "from distant_supervision.search import Search\n",
    "\n",
    "\n",
    "world = \"herox\"\n",
    "\n",
    "with open(\"data/distant_supervision/queries_\"+world+\".txt\", \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "    num_qs = len(lines)\n",
    "    done = 0\n",
    "    for line in lines:\n",
    "        done += 1\n",
    "        query = line.replace(\"\\n\",\" \").strip().split(\"\\t\")\n",
    "\n",
    "        table = query[0]\n",
    "        search = query[2]\n",
    "\n",
    "        if search.split(\"\\\" \\\"\")[1].replace(\"\\\"\",\"\").isnumeric():\n",
    "            print(\"skipped\")\n",
    "            print (query)\n",
    "        else:\n",
    "            try:\n",
    "                urls = Search.instance().search(search)\n",
    "\n",
    "                for url in urls:\n",
    "                    a = get_text(url)\n",
    "            except:\n",
    "                pass\n",
    "            print(str(100*done/num_qs) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Generation\n",
    "\n",
    "For each of the downloaded web pages. Parse the page and identify matches between the values in our tables and the data given in the web page. This only needs to be run once and will rememeber if it has been run before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from run.ds_generate_positive_features_for_query import precompute_features\n",
    "\n",
    "precompute_features(\"herox\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fact Checking\n",
    "\n",
    "### Training\n",
    "Load Modules for fact checking, generate the features and train our classifier from our training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from classifier.LogisticRegressionClassifier import LogisticRegressionClassifier\n",
    "from classifier.features.generate_features import FeatureGenerator, num, is_num\n",
    "from distant_supervision.utterance_detection import f_threshold_match\n",
    "from factchecking.question import Question\n",
    "from tabular.filtering import load_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fg = FeatureGenerator()\n",
    "Xs,ys = fg.generate_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier = LogisticRegressionClassifier()\n",
    "classifier.train(Xs,ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Runtime\n",
    "\n",
    "Load the source data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tables = load_collection(\"herox\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the fact checking function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fact_check(q):\n",
    "    question = Question(text=q, type=\"NUM\")\n",
    "    tuples,q_features = fg.generate_test(tables,question)\n",
    "\n",
    "    q_match = False\n",
    "\n",
    "    if len(tuples)>0:\n",
    "    \n",
    "        \n",
    "        \n",
    "        q_predicted = classifier.predict(q_features)\n",
    "\n",
    "        for i in range(len(tuples)):\n",
    "            tuple = tuples[i]\n",
    "            \n",
    "            skip = False\n",
    "            if 'date' in tuple[1].keys() and len(question.dates):\n",
    "                for date in question.dates:\n",
    "                    dstrs = set()\n",
    "                    for d in question.dates:\n",
    "                        dstrs.add(str(d))\n",
    "                    if not len(set(tuple[1]['date']).intersection(dstrs)):\n",
    "                        skip = True\n",
    "                        \n",
    "            if skip:\n",
    "                continue\n",
    "    \n",
    "\n",
    "            if is_num(tuple[1]['value']):\n",
    "                prediction = q_predicted[i]\n",
    "                features = q_features[i]\n",
    "\n",
    "                if prediction == 1:\n",
    "                    print(str(tuple) + \"\\t\\t\" + (\"Possible Match\" if prediction else \"No match\"))\n",
    "                    for number in question.numbers:\n",
    "                        value = num(tuple[1]['value'])\n",
    "\n",
    "                        if value is None:\n",
    "                            continue\n",
    "\n",
    "                        if f_threshold_match(number, value, 0.05):\n",
    "                            print(str(tuple) + \"\\t\\t\" + \"Threshold Match to 5%\")\n",
    "                            q_match = True\n",
    "\n",
    "                    for number in question.dates:\n",
    "                        value = num(tuple[1]['value'])\n",
    "                        if number == value:\n",
    "                            print(str(tuple) + \"\\t\\t\" + \"Exact Match\")\n",
    "                            q_match = True\n",
    "        print(question.text)\n",
    "        print(q_match)\n",
    "\n",
    "    else:\n",
    "        print(question.text)\n",
    "        print(\"No supporting information can be found in the knowledge base\")\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fact checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fact_check(\"Around 22250 unaccompanied children claimed asylum in Germany in 2015\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
