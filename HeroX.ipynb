{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "import os\n",
    "import re\n",
    "\n",
    "#Set path manually to incldue sources location\n",
    "if 'src/' not in sys.path:\n",
    "    sys.path.append('src/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load Java classpath for stanford corenlp using gradle. this will also install it if missing\n",
    "from subprocess import run,PIPE\n",
    "if 'CLASSPATH' not in os.environ:\n",
    "    if not (os.path.exists('build') and os.path.exists('build/classpath.txt')):\n",
    "        print(\"Generating classpath\")\n",
    "        r=run([\"./gradlew\", \"writeClasspath\"],stdout=PIPE, stderr=PIPE, universal_newlines=True)\n",
    "        print(r.stdout)\n",
    "        print(r.stderr)\n",
    "              \n",
    "    print(\"Loading classpath\")\n",
    "    os.environ['CLASSPATH'] = open('build/classpath.txt','r').read()\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from classifier.LogisticRegressionClassifier import LogisticRegressionClassifier\n",
    "from classifier.features.generate_features import FeatureGenerator, num\n",
    "from distant_supervision.utterance_detection import f_threshold_match\n",
    "from factchecking.question import Question\n",
    "from tabular.filtering import load_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: 0.0\n",
      "Search for \"Exxon Mobil\" \"Market Value\"\n",
      "Query already executed\n",
      "Done: 6.25\n",
      "Search for \"Unaccompanied children\" \"claimed asylum\"\n",
      "Query already executed\n",
      "Done: 12.5\n",
      "Search for \"Hamas\" \"Founded\"\n",
      "Query already executed\n",
      "Done: 18.75\n",
      "Search for \"United States\" \"Average Temperature\"\n",
      "Query already executed\n",
      "Done: 25.0\n",
      "Search for \"United States\" \"Life expectancy\"\n",
      "Query already executed\n",
      "Done: 31.25\n",
      "Search for \"United States\" \"Number of abortions\"\n",
      "Query already executed\n",
      "Done: 37.5\n",
      "Search for \"United States\" \"Abortion Rate per 1,000 births\"\n",
      "Query already executed\n",
      "Done: 43.75\n",
      "Search for \"United States Teenagers\" \"Percentage Enrolled in education\"\n",
      "Query already executed\n",
      "Done: 50.0\n",
      "Search for \"United States Teenagers\" \"Enrolled in education\"\n",
      "Query already executed\n",
      "Done: 56.25\n",
      "Search for \"America\" \"bee colonies 201\"\n",
      "Query already executed\n",
      "Done: 62.5\n",
      "Search for \"United States\" \"Financial Intermediary Funds 2016\"\n",
      "Query already executed\n",
      "Done: 68.75\n",
      "Search for \"United States\" \"Homocides by firearm\"\n",
      "Query already executed\n",
      "Done: 75.0\n",
      "Search for \"United Kingdom\" \"Educated at grammar schools 2016\"\n",
      "Query already executed\n",
      "Done: 81.25\n",
      "Search for \"USA\" \"Measles Vaccination Rate\"\n",
      "Query already executed\n",
      "Done: 87.5\n",
      "Search for \"USA\" \"Practicing doctors per 1000\"\n",
      "Query already executed\n",
      "Done: 93.75\n",
      "Search for \"USA\" \"Daily Smokers 2014\"\n",
      "Query already executed\n",
      "Registering words in BOW\n",
      "Registering columns\n",
      "Counting features\n",
      "Total positive: 54\n",
      "Total negative: 3278\n",
      "Looking for positive features\n",
      "0\n",
      "Subsampling negative features\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "fg = FeatureGenerator()\n",
    "Xs,ys = fg.generate_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier\n",
      "Trained\n"
     ]
    }
   ],
   "source": [
    "classifier = LogisticRegressionClassifier()\n",
    "classifier.train(Xs,ys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "register table herox/1.csv\n",
      "register table herox/2.csv\n",
      "register table herox/3.csv\n",
      "register table herox/4.csv\n",
      "register table herox/5.csv\n",
      "register table herox/8.csv\n",
      "register table herox/9.csv\n",
      "register table herox/10.csv\n",
      "register table herox/11.csv\n",
      "register table herox/12.csv\n",
      "register table herox/13.csv\n",
      "register table herox/14.csv\n"
     ]
    }
   ],
   "source": [
    "tables = load_collection(\"herox\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fact_check(q):\n",
    "    question = Question(text=q, type=\"NUM\")\n",
    "    tuples,q_features = fg.generate_test(tables,question)\n",
    "\n",
    "    q_match = False\n",
    "\n",
    "    if len(tuples)>0:\n",
    "        q_predicted = classifier.predict(q_features)\n",
    "\n",
    "        for i in range(len(tuples)):\n",
    "            tuple = tuples[i]\n",
    "            if len(tuple[1][2]) > 0:\n",
    "                prediction = q_predicted[i]\n",
    "                features = q_features[i]\n",
    "\n",
    "\n",
    "                if prediction == 1:\n",
    "                    print(str(tuple) + \"\\t\\t\" + (\"Possible Match\" if prediction else \"No match\"))\n",
    "                    for number in question.numbers:\n",
    "                        value = num(re.sub(r\"[^0-9\\.]+\", \"\", tuple[1][2].replace(\",\", \"\")))\n",
    "\n",
    "                        if value is None:\n",
    "                            continue\n",
    "\n",
    "                        if f_threshold_match(number, value, 0.05):\n",
    "                            print(str(tuple) + \"\\t\\t\" + \"Threshold Match to 5%\")\n",
    "                            q_match = True\n",
    "\n",
    "                    for number in question.dates:\n",
    "                        value = num(re.sub(r\"[^0-9\\.]+\", \"\", tuple[1][2].replace(\",\", \"\")))\n",
    "                        if number == value:\n",
    "                            print(str(tuple) + \"\\t\\t\" + \"Exact Match\")\n",
    "                            q_match = True\n",
    "        print(question.text)\n",
    "        print(q_match)\n",
    "\n",
    "    else:\n",
    "        print(question.text)\n",
    "        print(\"No supporting information can be found in the knowledge base\")\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('herox/11.csv', ('Firearm', 'USA', '11078'))\t\tPossible Match\n",
      "('herox/11.csv', ('Firearm', 'USA', '11078'))\t\tPossible Match\n",
      "In the USA 2014, the number of homicides by firearm was almost 10,000\n",
      "False\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fact_check(\"In the USA 2014, the number of homicides by firearm was almost 10,000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EU (28 countries)', 'Belgium', 'Bulgaria', 'Czech Republic', 'Denmark', 'Germany', 'Estonia', 'Ireland', 'Greece', 'Spain', 'France', 'Croatia', 'Italy', 'Cyprus', 'Latvia', 'Lithuania', 'Luxembourg', 'Hungary', 'Malta', 'Netherlands', 'Austria', 'Poland', 'Portugal', 'Romania', 'Slovenia', 'Slovakia', 'Finland', 'Sweden', 'United Kingdom', 'Iceland', 'Liechtenstein', 'Norway', 'Switzerland', 'Montenegro', 'Former Yugoslav Republic of Macedonia, the', 'Albania', 'Serbia', 'Turkey', '11695', '470', '15', '35', '300', '765', '0', '100', '295', '10', '410', ':', '575', '70', '5', '0', '0', '175', '20', '725', '695', '375', '5', '55', '20', '70', '705', '1510', '4285', '0', '0', '1045', '595', ':', ':', ':', ':', ':', '12190', '705', '10', '10', '520', '1305', '0', '55', '40', '20', '445', ':', '415', '20', '0', '5', '10', '270', '45', '1040', '1040', '360', '0', '40', '25', '30', '535', '2250', '2990', '0', '15', '1820', '415', ':', ':', ':', ':', ':', '10610', '860', '20', '5', '410', '1950', '0', '35', '145', '15', '610', ':', '305', '35', '5', '10', '20', '150', '5', '700', '600', '230', '5', '35', '25', '5', '315', '2395', '1715', '0', '0', '630', '220', ':', ':', ':', ':', ':', '11690', '1385', '25', '10', '270', '2125', '0', '25', '60', '10', '595', ':', '825', '15', '0', '10', '20', '60', '25', '485', '1005', '405', '5', '55', '60', '20', '150', '2655', '1395', '0', '0', '635', '310', ':', ':', ':', ':', ':', '12540', '975', '60', '5', '355', '2095', '0', '25', '75', '15', '490', '70', '970', '25', '0', '5', '15', '185', '105', '380', '1375', '245', '10', '135', '50', '5', '165', '3575', '1125', '5', '0', '705', '495', ':', ':', ':', ':', ':', '12725', '415', '185', '0', '350', '2485', '5', '20', '325', '10', '365', '55', '805', '55', '5', '0', '45', '380', '335', '310', '935', '255', '55', '15', '30', '5', '160', '3850', '1265', '0', '0', '670', '355', ':', ':', ':', ':', ':', '23150', '470', '940', '5', '815', '4400', '0', '30', '440', '15', '270', '10', '2505', '50', '0', '5', '30', '605', '55', '960', '1975', '185', '15', '95', '65', '10', '195', '7045', '1945', '0', '0', '940', '775', ':', ':', ':', ':', ':', '96465', '2850', '1815', '15', '2125', '22255', '0', '35', '420', '25', '320', '5', '4070', '105', '10', '5', '105', '8805', '35', '3855', '8275', '150', '50', '55', '40', '5', '2535', '35250', '3255', '5', '5', '5050', '2670', ':', ':', ':', ':', ':', 'Asylum applicants considered to be unaccompanied minors', 'Asylum applicants considered to be unaccompanied minors', 'Asylum applicants considered to be unaccompanied minors', 'Asylum applicants considered to be unaccompanied minors', 'Asylum applicants considered to be unaccompanied minors', 'Asylum applicants considered to be unaccompanied minors', 'Asylum applicants considered to be unaccompanied minors', 'Asylum applicants considered to be unaccompanied minors', 'Asylum applicants considered to be unaccompanied minors', 'Asylum applicants considered to be unaccompanied minors', 'Asylum applicants considered to be unaccompanied minors', 'Asylum applicants considered to be unaccompanied minors', 'Asylum applicants considered to be unaccompanied minors', 'Asylum applicants considered to be unaccompanied minors', 'Asylum applicants considered to be unaccompanied minors', 'Asylum applicants considered to be unaccompanied minors', 'Asylum applicants considered to be unaccompanied minors', 'Asylum applicants considered to be unaccompanied minors', 'Asylum applicants considered to be unaccompanied minors', 'Asylum applicants considered to be unaccompanied minors', 'Asylum applicants considered to be unaccompanied minors', 'Asylum applicants considered to be unaccompanied minors', 'Asylum applicants considered to be unaccompanied minors', 'Asylum applicants considered to be unaccompanied minors', 'Asylum applicants considered to be unaccompanied minors', 'Asylum applicants considered to be unaccompanied minors', 'Asylum applicants considered to be unaccompanied minors', 'Asylum applicants considered to be unaccompanied minors', 'Asylum applicants considered to be unaccompanied minors', 'Asylum applicants considered to be unaccompanied minors', 'Asylum applicants considered to be unaccompanied minors', 'Asylum applicants considered to be unaccompanied minors', 'Asylum applicants considered to be unaccompanied minors', 'Asylum applicants considered to be unaccompanied minors', 'Asylum applicants considered to be unaccompanied minors', 'Asylum applicants considered to be unaccompanied minors', 'Asylum applicants considered to be unaccompanied minors', 'Asylum applicants considered to be unaccompanied minors', '11695', '470', '15', '35', '300', '765', '0', '100', '295', '10', '410', ':', '575', '70', '5', '0', '0', '175', '20', '725', '695', '375', '5', '55', '20', '70', '705', '1510', '4285', '0', '0', '1045', '595', ':', ':', ':', ':', ':', '12190', '705', '10', '10', '520', '1305', '0', '55', '40', '20', '445', ':', '415', '20', '0', '5', '10', '270', '45', '1040', '1040', '360', '0', '40', '25', '30', '535', '2250', '2990', '0', '15', '1820', '415', ':', ':', ':', ':', ':', '10610', '860', '20', '5', '410', '1950', '0', '35', '145', '15', '610', ':', '305', '35', '5', '10', '20', '150', '5', '700', '600', '230', '5', '35', '25', '5', '315', '2395', '1715', '0', '0', '630', '220', ':', ':', ':', ':', ':', '11690', '1385', '25', '10', '270', '2125', '0', '25', '60', '10', '595', ':', '825', '15', '0', '10', '20', '60', '25', '485', '1005', '405', '5', '55', '60', '20', '150', '2655', '1395', '0', '0', '635', '310', ':', ':', ':', ':', ':', '12540', '975', '60', '5', '355', '2095', '0', '25', '75', '15', '490', '70', '970', '25', '0', '5', '15', '185', '105', '380', '1375', '245', '10', '135', '50', '5', '165', '3575', '1125', '5', '0', '705', '495', ':', ':', ':', ':', ':', '12725', '415', '185', '0', '350', '2485', '5', '20', '325', '10', '365', '55', '805', '55', '5', '0', '45', '380', '335', '310', '935', '255', '55', '15', '30', '5', '160', '3850', '1265', '0', '0', '670', '355', ':', ':', ':', ':', ':', '23150', '470', '940', '5', '815', '4400', '0', '30', '440', '15', '270', '10', '2505', '50', '0', '5', '30', '605', '55', '960', '1975', '185', '15', '95', '65', '10', '195', '7045', '1945', '0', '0', '940', '775', ':', ':', ':', ':', ':', '96465', '2850', '1815', '15', '2125', '22255', '0', '35', '420', '25', '320', '5', '4070', '105', '10', '5', '105', '8805', '35', '3855', '8275', '150', '50', '55', '40', '5', '2535', '35250', '3255', '5', '5', '5050', '2670', ':', ':', ':', ':', ':']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def read_table(filename,base=\"data/WikiTableQuestions\"):\n",
    "    header = []\n",
    "    rows = []\n",
    "\n",
    "    header_read = False\n",
    "    filename = filename.replace(\".csv\",\".tsv\")\n",
    "    with open(base+\"/\"+filename,\"r\",encoding='utf-8') as table:\n",
    "        has_header = csv.Sniffer().has_header(table.readline())\n",
    "        table.seek(0)\n",
    "\n",
    "        for line in csv.reader(table, delimiter=\"\\t\"):\n",
    "            if has_header and not header_read:\n",
    "                header = line\n",
    "                header_read = True\n",
    "            else:\n",
    "                rows.append(line)\n",
    "    return {\"header\": header, \"rows\":rows}\n",
    "\n",
    "def table_nes(table):\n",
    "    header = table['header']\n",
    "    rows = table['rows']\n",
    "\n",
    "    ret_tokens = []\n",
    "    for col in transpose(rows):\n",
    "        text = \". \".join(col)\n",
    "        doc = Annotation(text)\n",
    "        SharedNERPipeline().getInstance().annotate(doc)\n",
    "\n",
    "\n",
    "        num_ne_cell = 0\n",
    "        tokens = []\n",
    "        for cell in range(doc.get(CoreAnnotations.SentencesAnnotation).size()):\n",
    "            col = doc.get(CoreAnnotations.SentencesAnnotation).get(cell)\n",
    "\n",
    "            words = []\n",
    "            col_ne_tags = []\n",
    "            has_ne = False\n",
    "            for i in range(col.get(CoreAnnotations.TokensAnnotation).size()):\n",
    "                corelabel = col.get(CoreAnnotations.TokensAnnotation).get(i)\n",
    "                ne =corelabel.get(CoreAnnotations.NamedEntityTagAnnotation)\n",
    "\n",
    "                words.append(corelabel.get(CoreAnnotations.TextAnnotation))\n",
    "                if ne not in ['O','NUMBER','NUMERIC']:\n",
    "                    has_ne = True\n",
    "\n",
    "            if len(words) > 1:\n",
    "                tokens.append(\" \".join(words[:-1]))\n",
    "\n",
    "            if has_ne:\n",
    "                num_ne_cell += 1\n",
    "\n",
    "        if num_ne_cell >= len(tokens)/2 and len(tokens) > 0:\n",
    "            ret_tokens.extend(tokens)\n",
    "\n",
    "    return ret_tokens\n",
    "\n",
    "\n",
    "def number_tuples(table):\n",
    "    header = table['header']\n",
    "    rows = table['rows']\n",
    "\n",
    "    ret_tokens = []\n",
    "    entity_col = []\n",
    "    date_col = []\n",
    "    number_col = []\n",
    "    \n",
    "    col_id = 0\n",
    "    \n",
    "    table_trans = transpose(rows)\n",
    "    for col in table_trans:\n",
    "        \n",
    "        text = \". \".join(col)\n",
    "        doc = Annotation(text)\n",
    "        SharedNERPipeline().getInstance().annotate(doc)\n",
    "\n",
    "        num_ne_cell = 0\n",
    "        num_date_cell = 0\n",
    "        num_number_cell = 0\n",
    "        \n",
    "        tokens = []\n",
    "        for cell in range(doc.get(CoreAnnotations.SentencesAnnotation).size()):\n",
    "            col = doc.get(CoreAnnotations.SentencesAnnotation).get(cell)\n",
    "\n",
    "            words = []\n",
    "            col_ne_tags = []\n",
    "            has_ne = False\n",
    "            has_date = False\n",
    "            has_number = False\n",
    "            for i in range(col.get(CoreAnnotations.TokensAnnotation).size()):\n",
    "                corelabel = col.get(CoreAnnotations.TokensAnnotation).get(i)\n",
    "                ne =corelabel.get(CoreAnnotations.NamedEntityTagAnnotation)\n",
    "\n",
    "                words.append(corelabel.get(CoreAnnotations.TextAnnotation))\n",
    "                if ne not in ['O','NUMBER','NUMERIC','DATE','YEAR']:\n",
    "                    has_ne = True\n",
    "\n",
    "                if ne in ['YEAR','DATE']:    \n",
    "                    has_date = True\n",
    "                    \n",
    "                if ne in['NUMBER','NUMERIC','PERCENTAGE','ORDINAL']:\n",
    "                    has_number = True\n",
    "                    \n",
    "                    \n",
    "            if len(words) > 1:\n",
    "                tokens.append(\" \".join(words[:-1]))\n",
    "\n",
    "            if has_ne:\n",
    "                num_ne_cell += 1\n",
    "\n",
    "            if has_date:\n",
    "                num_date_cell += 1\n",
    "                \n",
    "            if has_number:\n",
    "                num_number_cell += 1\n",
    "        \n",
    "        if num_ne_cell >= len(tokens)/2 and len(tokens) > 0:\n",
    "            entity_col.append(col_id)\n",
    "\n",
    "        if num_date_cell >= len(tokens)/2 and len(tokens) > 0:\n",
    "            number_col.append(col_id)\n",
    "\n",
    "        if num_number_cell >= len(tokens)/2 and len(tokens) > 0:\n",
    "            number_col.append(col_id)\n",
    "\n",
    "        col_id +=1\n",
    "\n",
    "    \n",
    "    tuples = []\n",
    "    for col in entity_col:            \n",
    "        for col1 in number_col:\n",
    "\n",
    "            tuples.extend(list(zip([header[col1]] * len(rows),table_trans[col],table_trans[col1])))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return tuples\n",
    "\n",
    "\n",
    "\n",
    "def number_entity_tuples(table):\n",
    "    header = table['header']\n",
    "    rows = table['rows']\n",
    "\n",
    "    text = \". \".join(\" \".join(cell for cell in row) for row in transpose(rows))\n",
    "\n",
    "    doc = Annotation(text)\n",
    "    SharedNERPipeline().getInstance().annotate(doc)\n",
    "\n",
    "    ne_columns = []\n",
    "    number_columns = []\n",
    "    for column in range(doc.get(CoreAnnotations.SentencesAnnotation).size()):\n",
    "        col = doc.get(CoreAnnotations.SentencesAnnotation).get(column)\n",
    "\n",
    "\n",
    "        tokens = []\n",
    "        col_ne_tags = []\n",
    "        for i in range(col.get(CoreAnnotations.TokensAnnotation).size()):\n",
    "            corelabel = col.get(CoreAnnotations.TokensAnnotation).get(i)\n",
    "            tokens.append(corelabel.get(CoreAnnotations.TextAnnotation))\n",
    "            col_ne_tags.append(corelabel.get(CoreAnnotations.NamedEntityTagAnnotation))\n",
    "\n",
    "        tags = col_ne_tags\n",
    "\n",
    "\n",
    "        for tag in tags:\n",
    "            if len(set(col_ne_tags).intersection(set(number_ne_types))) == 0 and tag not in ['NUMBER','NUMERIC','YEAR','DATE','DURATION','TIME','NUMBER','ORDINAL'] and tag != \"O\":\n",
    "                ne_columns.append(column)\n",
    "                break\n",
    "\n",
    "\n",
    "        count_in = 0\n",
    "        if column not in ne_columns:\n",
    "            for tag in tags:\n",
    "                if tag in number_ne_types:\n",
    "                    count_in += 1\n",
    "\n",
    "            if count_in >= len(tags)/2:\n",
    "                number_columns.append(column)\n",
    "\n",
    "    numbers = []\n",
    "\n",
    "\n",
    "    tuples = []\n",
    "    transposed = transpose(rows)\n",
    "    for column in range(len(transposed)):\n",
    "        if column in ne_columns:\n",
    "            for ncolumn in range(len(transposed)):\n",
    "                if ncolumn in number_columns:\n",
    "                    tuples.extend(list(zip([header[ncolumn]] * len(rows),transposed[column],transposed[ncolumn])))\n",
    "\n",
    "\n",
    "    return tuples\n",
    "\n",
    "\n",
    "def number_entity_date_tuples(table):\n",
    "    header = table['header']\n",
    "    rows = table['rows']\n",
    "\n",
    "    ret_tokens = []\n",
    "    entity_col = []\n",
    "    date_col = []\n",
    "    number_col = []\n",
    "    \n",
    "    col_id = 0\n",
    "    \n",
    "    table_trans = transpose(rows)\n",
    "    \n",
    "   \n",
    "    hnums = set()\n",
    "    hidx = 0\n",
    "    for h in header:\n",
    "        doc = Annotation(h)\n",
    "        SharedNERPipeline().getInstance().annotate(doc)\n",
    "        \n",
    "        \n",
    "        for s in range(doc.get(CoreAnnotations.SentencesAnnotation).size()):\n",
    "            c = doc.get(CoreAnnotations.SentencesAnnotation).get(s)\n",
    "            for i in range(c.get(CoreAnnotations.TokensAnnotation).size()):\n",
    "                corelabel = c.get(CoreAnnotations.TokensAnnotation).get(i)\n",
    "                ne =corelabel.get(CoreAnnotations.NamedEntityTagAnnotation)\n",
    "                \n",
    "                if ne in ['YEAR','DATE']:\n",
    "                    hnums.add(hidx)\n",
    "        hidx+=1\n",
    "\n",
    "\n",
    "    hseries = False\n",
    "    if(len(hnums)>=len(header)/2):\n",
    "        hseries = True\n",
    "        \n",
    "        \n",
    "    for col in table_trans:\n",
    "        \n",
    "        text = \". \".join(col)\n",
    "        doc = Annotation(text)\n",
    "        SharedNERPipeline().getInstance().annotate(doc)\n",
    "\n",
    "\n",
    "        num_ne_cell = 0\n",
    "        num_date_cell = 0\n",
    "        num_number_cell = 0\n",
    "        \n",
    "        tokens = []\n",
    "        for cell in range(doc.get(CoreAnnotations.SentencesAnnotation).size()):\n",
    "            col = doc.get(CoreAnnotations.SentencesAnnotation).get(cell)\n",
    "\n",
    "            words = []\n",
    "            col_ne_tags = []\n",
    "            has_ne = False\n",
    "            has_date = False\n",
    "            has_number = False\n",
    "            for i in range(col.get(CoreAnnotations.TokensAnnotation).size()):\n",
    "                corelabel = col.get(CoreAnnotations.TokensAnnotation).get(i)\n",
    "                ne =corelabel.get(CoreAnnotations.NamedEntityTagAnnotation)\n",
    "\n",
    "                words.append(corelabel.get(CoreAnnotations.TextAnnotation))\n",
    "                if ne not in ['O','NUMBER','NUMERIC','DATE','YEAR']:\n",
    "                    has_ne = True\n",
    "\n",
    "                if ne in ['YEAR','DATE']:    \n",
    "                    has_date = True\n",
    "                    \n",
    "                if ne in['NUMBER','NUMERIC','PERCENTAGE','ORDINAL']:\n",
    "                    has_number = True\n",
    "                    \n",
    "                    \n",
    "            if len(words) > 1:\n",
    "                tokens.append(\" \".join(words[:-1]))\n",
    "\n",
    "            if has_ne:\n",
    "                num_ne_cell += 1\n",
    "\n",
    "            if has_date:\n",
    "                num_date_cell += 1\n",
    "                \n",
    "            if has_number:\n",
    "                num_number_cell += 1\n",
    "        \n",
    "        if not hseries:\n",
    "            if num_ne_cell >= len(tokens)/2 and len(tokens) > 0:\n",
    "                entity_col.append(col_id)\n",
    "\n",
    "            if num_date_cell >= len(tokens)/2 and len(tokens) > 0:\n",
    "                date_col.append(col_id)\n",
    "\n",
    "            if num_number_cell >= len(tokens)/2 and len(tokens) > 0:\n",
    "                number_col.append(col_id)\n",
    "                \n",
    "        \n",
    "\n",
    "        col_id +=1\n",
    "\n",
    "    \n",
    "    tuples = []\n",
    "    \n",
    "    if not hseries:\n",
    "        for col in entity_col:            \n",
    "            for col1 in number_col:\n",
    "                extra = []\n",
    "                if len(date_col)>0:\n",
    "                    for dc in date_col:\n",
    "                        extra.append(table_trans[dc])\n",
    "                    extra = transpose(extra)\n",
    "                    tuples.extend(list(zip([header[col1]] * len(rows),table_trans[col],table_trans[col1],extra)))\n",
    "                else:\n",
    "                    tuples.extend(list(zip([header[col1]] * len(rows),table_trans[col],table_trans[col1])))\n",
    "\n",
    "    else:\n",
    "        nh = (set(range(len(header))).difference(hnums))\n",
    "        tr = []\n",
    "        for col in nh:\n",
    "            tr.extend(table_trans[col])\n",
    "            for col in hnums:\n",
    "                tr.extend(table_trans[col])\n",
    "        \n",
    "        tuples.extend(tr)\n",
    "        \n",
    "    \n",
    "    \n",
    "    return tuples\n",
    "\n",
    "\n",
    "def transpose(l):\n",
    "    return list(map(list, zip(*l)))\n",
    "\n",
    "t = read_table(\"herox/2.csv\")\n",
    "print(number_entity_date_tuples(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
