{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# HeroX Numerical Fact Checking System. Univeristy of Sheffield\n",
    "\n",
    "Pre-requisites:\n",
    " * Gradle\n",
    " * Java jdk8\n",
    " * Python 3\n",
    "  * numpy\n",
    "  * jnius\n",
    "  * fuzzywuzzy\n",
    "  * sklearn\n",
    "  * urllib3\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common setup\n",
    "\n",
    "Import required dependencies and download/install Stanford CoreNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "\n",
    "#Set path manually to incldue sources location\n",
    "if 'src/' not in sys.path:\n",
    "    sys.path.append('src/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the following step fails. Run `gradlew writeClasspath` on the terminal in this folder. Then try again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading classpath\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "#Load Java classpath for stanford corenlp using gradle. this will also install it if missing\n",
    "from subprocess import run,PIPE\n",
    "if 'CLASSPATH' not in os.environ:\n",
    "    if not (os.path.exists('build') and os.path.exists('build/classpath.txt')):\n",
    "        print(\"Generating classpath\")\n",
    "        r=run([\"./gradlew\", \"writeClasspath\"],stdout=PIPE, stderr=PIPE, universal_newlines=True)\n",
    "        print(r.stdout)\n",
    "        print(r.stderr)\n",
    "              \n",
    "    print(\"Loading classpath\")\n",
    "    os.environ['CLASSPATH'] = open('build/classpath.txt','r').read()\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Optional] Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Search Engine Queries From Tables\n",
    "\n",
    "Run if a new table is added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from distant_supervision.query_generation import generate_queries\n",
    "from tabular.table_reader import read_table, number_tuples\n",
    "from wikitablequestions.dataset_reader import load_instances\n",
    "\n",
    "print(\"Generating Queries\")\n",
    "world = \"herox\"\n",
    "if os.path.exists(\"data/distant_supervision/queries_\"+world +\".txt\"):\n",
    "    print(\"Already done. No need to run again\")\n",
    "else:\n",
    "    all_instances = []\n",
    "    all_instances.extend(load_instances(world))\n",
    "    table_files = []\n",
    "\n",
    "    done = 0\n",
    "    for instance in all_instances:\n",
    "        table_files.append(instance['table'])\n",
    "\n",
    "    table_files = set(table_files)\n",
    "\n",
    "    with open(\"data/distant_supervision/queries_\"+world +\".txt\", \"w+\") as file:\n",
    "        for table_file in table_files:\n",
    "            done += 1\n",
    "            print(\"Parsing \" + str(done) +\"/\"+str(len(table_files)) + \"\\t\\t\\t\" + table_file)\n",
    "            table = number_tuples(read_table(table_file))\n",
    "            tuples = generate_queries(table)\n",
    "\n",
    "\n",
    "            for tuple in tuples:\n",
    "                file.write(table_file + \"\\t\" + tuple + \"\\n\")\n",
    "            file.flush()\n",
    "            os.fsync(file.fileno())\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download all web pages for queries\n",
    "\n",
    "We include the web pages with this submission as downloading the web pages takes considerable time. This script will not re-download web pages it already has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from distant_supervision.clean_html import get_text\n",
    "from distant_supervision.search import Search\n",
    "\n",
    "\n",
    "world = \"herox\"\n",
    "\n",
    "with open(\"data/distant_supervision/queries_\"+world+\".txt\", \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "    num_qs = len(lines)\n",
    "    done = 0\n",
    "    for line in lines:\n",
    "        done += 1\n",
    "        query = line.replace(\"\\n\",\" \").strip().split(\"\\t\")\n",
    "\n",
    "        table = query[0]\n",
    "        search = query[2]\n",
    "\n",
    "        if search.split(\"\\\" \\\"\")[1].replace(\"\\\"\",\"\").isnumeric():\n",
    "            print(\"skipped\")\n",
    "            print (query)\n",
    "        else:\n",
    "            try:\n",
    "                urls = Search.instance().search(search)\n",
    "\n",
    "                for url in urls:\n",
    "                    a = get_text(url)\n",
    "            except:\n",
    "                pass\n",
    "            print(str(100*done/num_qs) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Generation\n",
    "\n",
    "For each of the downloaded web pages. Parse the page and identify matches between the values in our tables and the data given in the web page. This only needs to be run once and will rememeber if it has been run before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/james/anaconda/lib/python3.5/site-packages/fuzzywuzzy/fuzz.py:35: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 1 out of 10625\n",
      "Search for \"Barbados\" GDP (current US$)  2012\n",
      "Query already executed\n",
      "Done URL 1 out of 50\n",
      " \n",
      "Looking in document for values similar to 4332141067\n",
      "https://www.bing.com/cr?IG=A7362BC59D6B43288271323AA77D8E1C&CID=33B946C9F5416F5834104F39F4A66ED2&rd=1&h=oG7kld5RbInomlQ7EtWICQ3PW05C4Jgbr9DeVdgvvIw&v=1&r=https%3a%2f%2fen.wikipedia.org%2fwiki%2fEconomy_of_Barbados&p=DevEx,5132.1\n",
      "Done URL 2 out of 50\n",
      " \n",
      "Looking in document for values similar to 4332141067\n",
      "http://www.bing.com/cr?IG=A7362BC59D6B43288271323AA77D8E1C&CID=33B946C9F5416F5834104F39F4A66ED2&rd=1&h=uolkG39uS4CM-mAIVBqaLsqV6i41opiQRb8f6jyKbio&v=1&r=http%3a%2f%2fdata.worldbank.org%2fcountry%2fbarbados&p=DevEx,5145.1\n",
      "No meaningful text in this document\n",
      "Done URL 3 out of 50\n",
      " \n",
      "Looking in document for values similar to 4332141067\n",
      "http://www.bing.com/cr?IG=A7362BC59D6B43288271323AA77D8E1C&CID=33B946C9F5416F5834104F39F4A66ED2&rd=1&h=xHeDJ4f_6UqhWdAkGiv5cfQrPJWhvQAN2e75A_XNce4&v=1&r=http%3a%2f%2fwww.heritage.org%2findex%2fcountry%2fbarbados&p=DevEx,5159.1\n",
      "8 candidate matches\n",
      "['Prime Minister Freundel Stuart , whose Democratic Labour Party won a five-year term in 2013 , has announced that he wants the Caribbean island to become a republic in 2016 , the 50th anniversary of its independence from the United Kingdom .', \"Barbados has transformed itself from a low-income agricultural economy producing mainly sugar and rum into a middle-income economy built on tourism and offshore banking that generates one of the Caribbean 's highest per capita incomes .\", 'Fiscal consolidation measures adopted since 2014 have helped to stabilize the economy and contributed to a slight rebound in growth in 2015 .', 'The top income tax rate is 35 percent , and the top corporate tax rate is 25 percent .', 'The overall tax burden amounts to 23.9 percent of total domestic income .', 'Expansionary government spending has climbed to over 45 percent of total domestic output .', 'With chronically high budget deficits averaging over 4 percent of GDP , public debt has risen to about the size of the economy .', 'Barbados has a relatively high 13.9 percent average tariff rate .']\n",
      "About to parse\n"
     ]
    }
   ],
   "source": [
    "from run.ds_generate_positive_features_for_query import precompute_features\n",
    "\n",
    "precompute_features(\"emnlp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fact Checking\n",
    "\n",
    "### Training\n",
    "Load Modules for fact checking, generate the features and train our classifier from our training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/james/anaconda/lib/python3.5/site-packages/fuzzywuzzy/fuzz.py:35: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "from classifier.Classifier import Classifier\n",
    "from classifier.LogisticRegressionClassifier import LogisticRegressionClassifier\n",
    "from classifier.features.generate_features import FeatureGenerator, num, is_num\n",
    "from distant_supervision.utterance_detection import f_threshold_match\n",
    "from factchecking.question import Question\n",
    "from tabular.filtering import load_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: 0.0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-33738db50f6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFeatureGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mXs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"herox\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/james/Dropbox/phd/Year 1/projects/3 Verification of Nmerical Claims EACL/code/Numerical-Fact-Checking/src/classifier/features/generate_features.py\u001b[0m in \u001b[0;36mgenerate_training\u001b[0;34m(self, world, maxno)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m                 \u001b[0msearch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "fg = FeatureGenerator()\n",
    "Xs,ys = fg.generate_training(\"herox\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "class LogisticRegressionClassifier(Classifier):\n",
    "    def train(self, Xs, ys):\n",
    "        print(\"Training classifier 3\")\n",
    "        self.lr = LogisticRegression(penalty='l1', C=0.78)\n",
    "        self.lr.fit(Xs, ys)\n",
    "        print(\"Trained\")\n",
    "\n",
    "    def predict(self, q_features):\n",
    "        ys = (self.lr.predict(q_features), self.lr.predict_proba(q_features))\n",
    "        return ys\n",
    "\n",
    "\n",
    "classifier = LogisticRegressionClassifier()\n",
    "classifier.train(Xs,ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Runtime\n",
    "\n",
    "Load the source data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tables = load_collection(\"emnlp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the fact checking function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fact_check(q):\n",
    "    question = Question(text=q, type=\"NUM\")\n",
    "    question.parse()\n",
    "    tuples,q_features = fg.generate_test(tables,question)\n",
    "    q_match = False\n",
    "    \n",
    "  \n",
    "        \n",
    "    if len(tuples)>0:\n",
    "  \n",
    "    \n",
    "        \n",
    "        \n",
    "        q_predicted = classifier.predict(q_features)\n",
    "\n",
    "        for i in range(len(tuples)):\n",
    "            tuple = tuples[i]\n",
    "            \n",
    "            skip = False\n",
    "            if 'date' in tuple[1].keys() and len(question.dates):\n",
    "                for date in question.dates:\n",
    "                    dstrs = set()\n",
    "                    for d in question.dates:\n",
    "                        dstrs.add(str(d))\n",
    "                    if not len(set(tuple[1]['date']).intersection(dstrs)):\n",
    "                        skip = True\n",
    "                        \n",
    "            if skip:\n",
    "                continue\n",
    "    \n",
    "\n",
    "            if is_num(tuple[1]['value']):\n",
    "                prediction = q_predicted[0][i]\n",
    "                features = q_features[i]\n",
    "\n",
    "                \n",
    "             \n",
    "                if prediction == 1:\n",
    "                    print(str(tuple) + \"\\t\\t\" + (\"Possible Match\" if prediction else \"No match\"))\n",
    "                    for number in question.numbers:\n",
    "                        value = num(tuple[1]['value'])\n",
    "\n",
    "                        if value is None:\n",
    "                            continue\n",
    "\n",
    "                        if f_threshold_match(number, value, 0.05):\n",
    "                            print(str(tuple) + \"\\t\\t\" + \"Threshold Match to 5%\")\n",
    "                            q_match = True\n",
    "\n",
    "                    for number in question.dates:\n",
    "                        value = num(tuple[1]['value'])\n",
    "                        if number == value:\n",
    "                            print(str(tuple) + \"\\t\\t\" + \"Exact Match\")\n",
    "                            q_match = True\n",
    "        print(question.text)\n",
    "        print(q_match)\n",
    "\n",
    "    else:\n",
    "        print(question.text)\n",
    "        print(\"No supporting information can be found in the knowledge base\")\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "fact_check(\"Germany had a population of 80 million people residing there in 2015\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fact checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fact_check(\"Germany's GDP is growth is 10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "base = \"/Users/james/Dropbox/Fact Checking/james\"\n",
    "rels = []\n",
    "from distant_supervision.normalisation import normalise,normalise_keep_nos\n",
    "from collections import defaultdict\n",
    "import re \n",
    "import pickle\n",
    "import ast\n",
    "\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "\n",
    "def normalise(text):\n",
    "    text = text.replace(\"lrb\",\"\")\n",
    "    text = text.replace(\"lsb\", \"\")\n",
    "    text = text.replace(\"rrb\", \"\")\n",
    "    text = text.replace(\"rsb\", \"\")\n",
    "    \n",
    "    text = text.replace(\"-lrb-\",\"\")\n",
    "    text = text.replace(\"-lsb-\", \"\")\n",
    "    text = text.replace(\"-rrb-\", \"\")\n",
    "    text = text.replace(\"-rsb-\", \"\")\n",
    "    \n",
    "    text = text.replace(\"-LRB-\",\"\")\n",
    "    text = text.replace(\"-LSB-\", \"\")\n",
    "    text = text.replace(\"-RRB-\", \"\")\n",
    "    text = text.replace(\"-LSB-\", \"\")\n",
    "\n",
    "    text = re.sub(r'[^\\w]', ' ', text)\n",
    "    text = re.sub(r'[0-9]','D', text.lower())\n",
    "    return text\n",
    "\n",
    "def fact_check_and_test(q, rel):\n",
    "    question = q # Question(text=q, type=\"NUM\")\n",
    "    tuples, q_features = fg.generate_test(tables, question)\n",
    "    q_match = False\n",
    "\n",
    "    matches = dict()\n",
    "    \n",
    "    p_match = 0.0\n",
    "    found_match = False \n",
    "    total_geq = 0\n",
    "    total_gt = 0\n",
    "    total_match= 0\n",
    "    \n",
    "    entities = set()\n",
    "    if len(tuples) > 0:\n",
    "        for i in range(len(tuples)):\n",
    "            \n",
    "            tuple = tuples[i]\n",
    "            skip = False\n",
    "            if 'date' in tuple[1].keys() and len(question.dates):\n",
    "                for date in question.dates:\n",
    "                    dstrs = set()\n",
    "                    for d in question.dates:\n",
    "                        dstrs.add(str(d))\n",
    "                    if not len(set(tuple[1]['date']).intersection(dstrs)):\n",
    "                        skip = True\n",
    "            \n",
    "            if skip or not is_num(tuple[1]['value']):\n",
    "                continue\n",
    "            \n",
    "            entities.add(tuple[1]['entity'])\n",
    "            matches[tuple[1]['entity'] + \"-----\" + tuple[1]['relation']] = (tuple,q_features[i])\n",
    "\n",
    "    if len(matches.keys()) > 0:\n",
    "        for i in matches.keys():\n",
    "            \n",
    "            tuple = matches[i][0]\n",
    "          \n",
    "            features = matches[i][1]\n",
    "            q_predicted = classifier.predict([features])\n",
    "\n",
    "            skip = False\n",
    "            if 'date' in tuple[1].keys() and len(question.dates):\n",
    "                for date in question.dates:\n",
    "                    dstrs = set()\n",
    "                    for d in question.dates:\n",
    "                        dstrs.add(str(d))\n",
    "                    if not len(set(tuple[1]['date']).intersection(dstrs)):\n",
    "                        skip = True\n",
    "\n",
    "            if skip or not is_num(tuple[1]['value']):\n",
    "                continue\n",
    "\n",
    "            prediction = q_predicted[0][0]\n",
    "            \n",
    "            if prediction == 1:\n",
    "                \n",
    "                if (tuple[1]['relation'] == rel):\n",
    "                    p_match = q_predicted[1][0][1]\n",
    "                    found_match = True\n",
    "            \n",
    "    else:\n",
    "        return (-1,0,0)\n",
    "\n",
    "    \n",
    "    if found_match:\n",
    "        for i in matches.keys():\n",
    "            \n",
    "            tuple = matches[i][0]\n",
    "            features = matches[i][1]\n",
    "            q_predicted = classifier.predict([features])\n",
    "\n",
    "            \n",
    "            skip = False\n",
    "            if 'date' in tuple[1].keys() and len(question.dates):\n",
    "                for date in question.dates:\n",
    "                    dstrs = set()\n",
    "                    for d in question.dates:\n",
    "                        dstrs.add(str(d))\n",
    "                    if not len(set(tuple[1]['date']).intersection(dstrs)):\n",
    "                        skip = True\n",
    "\n",
    "            if skip or not is_num(tuple[1]['value']):\n",
    "                continue\n",
    "\n",
    "            prediction = q_predicted[0][0]\n",
    "           \n",
    "            if prediction == 1:\n",
    "                if not (tuple[1]['relation'] == rel):\n",
    "                    if q_predicted[1][0][1] > p_match:\n",
    "                        total_gt += 1\n",
    "                    if q_predicted[1][0][1] >= p_match:\n",
    "                        total_geq += 1\n",
    "                    total_match += 1\n",
    "                    \n",
    "        \n",
    "    if found_match:\n",
    "        print(\"matched - \")\n",
    "        print(total_gt)\n",
    "        print(total_geq)\n",
    "        print(total_match)\n",
    "        return (1,total_gt,total_match,total_geq)\n",
    "    \n",
    "    rs = set()\n",
    "    for tuple in tuples:\n",
    "        if not is_num(tuple[1]['value']):\n",
    "            pass\n",
    "        rs.add(tuple[1]['relation'])\n",
    "\n",
    "    if rel not in rs:\n",
    "        return (-1,0,0,0)\n",
    "    return (0,0,0,0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for filename in os.listdir(base):\n",
    "    if filename.endswith(\".tsv\"):\n",
    "        with open(base+\"/\"+filename,encoding = \"ISO-8859-1\") as tsv:\n",
    "            for line in tsv.readlines():\n",
    "                row = line.split(\"\\t\")\n",
    "                if(len(row) == 12) and len(row[5].strip())>0:\n",
    "                    if(row[0].lower().strip()=='y') or (row[1].lower().strip()=='y') :\n",
    "                        rels.append({\"claim\":row[2],\"relation\":row[5],\"entity\":row[3],\"num\":row[9],\"parsed\":row[8]})\n",
    "                elif len(row) == 11:\n",
    "                    if(row[0].lower().strip()=='y') and len(row[4].strip())>0:\n",
    "                        rels.append({\"claim\":row[1],\"relation\":row[4],\"entity\":row[2],\"num\":row[8],\"parsed\":row[7]})\n",
    "           \n",
    "                 \n",
    "    \n",
    "\n",
    "\n",
    "property_names = dict()\n",
    "\n",
    "property_names['fertility_rate'] = \"Fertility rate, total (births per woman)\"\n",
    "property_names['gdp_growth_rate'] = \"GDP growth (annual %)\"\n",
    "property_names['gdp_nominal'] = \"GDP (current US$)\"\n",
    "property_names['gdp_nominal_per_capita'] = \"GDP per capita (current US$)\"\n",
    "property_names['gni_per_capita_in_ppp_dollars'] = \"GNI per capita, PPP (current international $)\"\n",
    "property_names['life_expectancy'] = \"Life expectancy at birth, total (years)\"\n",
    "property_names['cpi_inflation_rate'] = \"Inflation, consumer prices (annual %)\"\n",
    "property_names['consumer_price_index'] = \"Consumer price index (2010 = 100)\"\n",
    "property_names['diesel_price_liter'] = \"Pump price for diesel fuel (US$ per liter)\"\n",
    "property_names['gni_in_ppp_dollars'] = \"GNI (current US$)\"\n",
    "property_names['population_growth_rate'] = \"Population growth (annual %)\"\n",
    "property_names['population'] = \"Population, total\"\n",
    "property_names['prevalence_of_undernourisment'] = \"Prevalence of undernourishment (% of population)\"\n",
    "property_names['renewable_freshwater_per_capita'] = \"Renewable internal freshwater resources per capita (cubic meters)\"\n",
    "property_names['health_expenditure_as_percent_of_gdp'] = \"Health expenditure, total (% of GDP)\"\n",
    "property_names['internet_users_percent_population'] = \"Internet users (per 100 people)\"\n",
    "\n",
    "tested = defaultdict(int)\n",
    "results = defaultdict(int)\n",
    "pr = defaultdict(int)\n",
    "\n",
    "num_better = defaultdict(int)\n",
    "num_total = defaultdict(int)\n",
    "num_better_or_equal = defaultdict(int)\n",
    "\n",
    "print(len(rels))\n",
    "\n",
    "\n",
    "claim_loc = re.compile(r'<location[^>]*>([^<]+)</location>')\n",
    "claim_num = re.compile(r'<number[^>]*>([^<]+)</number>')\n",
    "\n",
    "\n",
    "class NewQuestion():\n",
    "    def __init__(self,text,entity,number):\n",
    "        self.text = text\n",
    "        self.nes = {entity}\n",
    "        self.numbers = {num(number)}\n",
    "        self.dates = set()\n",
    "        self.nps = set()\n",
    "    def parse(self):\n",
    "        pass\n",
    "        \n",
    "qs = []\n",
    "        \n",
    "for rel in rels:\n",
    "    if len(claim_loc.findall(rel['claim'])) > 0:\n",
    "        rel['num'] = claim_num.findall(rel['claim'])[0]\n",
    "\n",
    "    start_claim_idx = rel['claim'].index(rel['entity'])\n",
    "    end_claim_idx = start_claim_idx + len(rel['entity'])\n",
    "\n",
    "\n",
    "    start_num_idx = rel['claim'].index(rel['num'])\n",
    "    end_num_idx = start_num_idx + len(rel['num'])\n",
    "\n",
    "    span = \"\"\n",
    "    if end_claim_idx < start_num_idx:\n",
    "        span = (rel['claim'][end_claim_idx:start_num_idx])\n",
    "    else:\n",
    "        span =(rel['claim'][start_num_idx:end_claim_idx])\n",
    "\n",
    "    span = re.sub('<[^<]+?>', '', span)\n",
    "    #print(normalise(span).split())\n",
    "    \n",
    "    spanwords = span.split()\n",
    "    \n",
    "    if(rel['parsed'][0]==\"\\\"\"):\n",
    "        rel['parsed'] = rel['parsed'][1:-1]\n",
    "        \n",
    "    dep_parse = ast.literal_eval(rel['parsed'])\n",
    "    \n",
    "    tokens = []\n",
    "    for token in dep_parse:\n",
    "        \n",
    "        for w in (token.replace(\"*extend*\",\"\").split(\"+\")):\n",
    "            we = w.split(\"~\")[0].replace(\"\\\"\",\"\")\n",
    "            \n",
    "            if \",\" in we:\n",
    "                for t in we.split(\",\"):\n",
    "                    if not(t  == \"NUMBER_SLOT\" or t == \"LOCATION_SLOT\"):\n",
    "                        tokens.append(t)\n",
    "            elif not(we  == \"NUMBER_SLOT\" or we == \"LOCATION_SLOT\"):\n",
    "                tokens.append(we)\n",
    "    tokens = \" \".join(tokens).replace(\"DATE\",\"1000\").replace(\"PERCENT\",\"10\").replace(\"MISC\",\"$\")\n",
    "    tokens += \" \"\n",
    "    tokens += \" \".join(spanwords).replace(\"DATE\",\"1000\").replace(\"PERCENT\",\"10\").replace(\"MISC\",\"$\")\n",
    "    \n",
    "    q = NewQuestion(rel['claim'],rel['entity'],rel['num'])\n",
    "    words = normalise_keep_nos(q.text).split()\n",
    "\n",
    "    qs.append((q,rel))\n",
    "\n",
    "    \n",
    "done= 0\n",
    "for item in qs:\n",
    "    done += 1\n",
    "    print(rel['claim'])\n",
    "    print(rel['relation'])\n",
    "    \n",
    "    rel = item[1]\n",
    "    q = item[0]\n",
    "    result = fact_check_and_test(q, property_names[rel['relation']])\n",
    "    \n",
    "    if result[0] == 1:\n",
    "        results[rel['relation']] += 1\n",
    "        if result[1] == 0:\n",
    "            pr[rel['relation']] += result[3]\n",
    "        num_better[rel['relation']] += result[1]\n",
    "        num_better_or_equal[rel['relation']] += result[3]    \n",
    "        num_total[rel['relation']] += result[2]\n",
    "\n",
    "    if result[0] != -1:\n",
    "        tested[rel['relation']] += 1\n",
    "\n",
    "    print(result)\n",
    "    print(\"done\" + str(done) )\n",
    "    print(\"\")\n",
    "    \n",
    "    if done%5000 == 0:\n",
    "        for key in tested.keys():\n",
    "            print(key + \" \" + str(results[key]) + \" \" + str(num_better[key]) + \" \" + str(num_better_or_equal[key]) + \" \" + str(num_total[key]) + \" \" + str(pr[key]) + \" \" + str(tested[key]) + \" \" + str(results[key] / tested[key]))\n",
    "\n",
    "print(\"Done\")\n",
    "for key in tested.keys():\n",
    "    print(key + \" \" + str(results[key]) + \" \" + str(num_better[key]) + \" \" + str(num_better_or_equal[key]) + \" \" + str(num_total[key]) + \" \" + str(pr[key]) + \" \" + str(tested[key]) + \" \" + str(results[key] / tested[key]))\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
